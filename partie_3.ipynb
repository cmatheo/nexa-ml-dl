{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Partie 3 : Analyse de Données Séquentielles avec le Deep Learning (NLP)",
   "id": "e39f1489634d17d1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3.1. Contexte et Objectifs\n",
    "Ce dernier module aborde les données séquentielles via le Traitement Automatique du Langage (NLP). L'ordre des mots étant crucial, nous utiliserons des architectures spécifiques comme les Réseaux de Neurones Récurrents (RNN) et leur variante avancée, les Long Short-Term Memory (LSTM), qui sont capables de mémoriser des informations sur de longues séquences.\n",
    "\n",
    "Votre objectif sera de :\n",
    "- Maîtriser le pipeline de prétraitement de texte avec `TextVectorization`.\n",
    "- Comprendre le rôle de la couche `Embedding` pour la représentation sémantique des mots.\n",
    "- Construire et entraîner un modèle de classification de texte basé sur une architecture `LSTM`."
   ],
   "id": "5dc1454a985122e9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3.2. Cas d'Usage : Catégorisation Automatique de Dépenses\n",
    "Pour ce projet, pourriez-vous utiliser un jeu de données de catégorisation de dépenses comme celui-ci sur Kaggle ? Il contient des descriptions textuelles de transactions (ex: \"STARBUCKS COFFEE\") et leur catégorie (ex: \"Food\") . Votre but est de prédire la catégorie à partir de la description."
   ],
   "id": "7a670c7f1d601701"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3.3. Implémentation Guidée",
   "id": "34f5e8bbc1331a75"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Étape 1 : Prétraitement du Texte\n",
    "L'approche moderne consiste à intégrer le prétraitement dans le modèle. Pourriez-vous utiliser la couche `tf.keras.layers.TextVectorization` pour standardiser, tokeniser et vectoriser le texte ?\n",
    "```python\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Charger les données (exemple avec le dataset de Kaggle)\n",
    "df = pd.read_csv('personal_expense_dataset.csv')\n",
    "\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "# Encoder les labels textuels en entiers\n",
    "label_encoder = LabelEncoder()\n",
    "df['category_encoded'] = label_encoder.fit_transform(df['Category'])\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Séparer les données\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df,\n",
    "    df['category_encoded'],\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Créer et adapter la couche de vectorisation\n",
    "max_features = 10000\n",
    "sequence_length = 50\n",
    "\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    max_tokens=max_features,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length\n",
    ")\n",
    "\n",
    "# Adapter la couche au vocabulaire des textes d'entraînement\n",
    "vectorize_layer.adapt(train_texts.values)\n",
    "```"
   ],
   "id": "f5229a19c128843"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Étape 2 : Construction du Modèle Séquentiel\n",
    "Votre tâche est maintenant de construire le modèle. Pourriez-vous utiliser la structure suivante ?\n",
    "1. La couche `TextVectorization` que vous venez de créer.\n",
    "2. Une couche `Embedding` pour apprendre des représentations vectorielles denses des mots.\n",
    "3. Une couche `Bidirectional(LSTM)` pour analyser la séquence dans les deux sens, capturant ainsi un contexte plus riche.\n",
    "4. Des couches `Dense` pour la classification finale.\n",
    "\n",
    "```python\n",
    "embedding_dim = 128\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Input(shape=(1,), dtype=tf.string),\n",
    "    vectorize_layer,\n",
    "    layers.Embedding(input_dim=max_features + 1, output_dim=embedding_dim),\n",
    "    layers.Bidirectional(layers.LSTM(64)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_classes, activation='softmax') # Softmax pour la classification multi-classe\n",
    "])\n",
    "```"
   ],
   "id": "a6c871279d321636"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Étape 3 : Callbacks, Compilation et Entraînement\n",
    "Comme pour la partie 1, pourriez-vous configurer des callbacks `ModelCheckpoint` et `EarlyStopping` ? Ensuite, compilez et entraînez le modèle.\n",
    "\n",
    "```python\n",
    "# Callbacks\n",
    "nlp_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='best_model_nlp.keras',\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True\n",
    ")\n",
    "\n",
    "nlp_early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Compilation\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Entraînement\n",
    "epochs = 10\n",
    "history = model.fit(\n",
    "    train_texts,\n",
    "    train_labels,\n",
    "    validation_data=(test_texts, test_labels),\n",
    "    epochs=epochs,\n",
    "    callbacks=[nlp_checkpoint_callback, nlp_early_stopping_callback]\n",
    ")\n",
    "```"
   ],
   "id": "bd6f387aaaba3b66"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
